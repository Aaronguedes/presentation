{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pyspark.sql.functions import col,lit, current_timestamp,concat,desc\n",
    "from centreon_apirest_transformations import get_token\n",
    "import requests\n",
    "import json \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##parametros de JOB no Databricks\n",
    "systemSource = dbutils.widgets.get(\"systemSource\")\n",
    "tableSource = dbutils.widgets.get(\"tableSource\")\n",
    "isIncremental = dbutils.widgets.get(\"isIncremental\")\n",
    "isSensitive = dbutils.widgets.get(\"isSensitive\")\n",
    "isDev = dbutils.widgets.get(\"isDev\")\n",
    "trigger = dbutils.widgets.get(\"trigger\")\n",
    "schemaName = dbutils.widgets.get(\"schemaName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert None not in [systemSource,tableSource,isDev,isSensitive,isIncremental,schemaName], \"None is not a valid input\"\n",
    "systemSource = str(systemSource).strip().lower()\n",
    "tableSource = str(tableSource).strip()\n",
    "isIncremental = str(isIncremental).strip().capitalize()\n",
    "isSensitive = str(isSensitive).strip().capitalize()\n",
    "isDev = str(isDev).strip().capitalize()\n",
    "trigger = str(trigger).strip().lower()\n",
    "schemaName = str(schemaName).strip().lower()\n",
    "assert bool(systemSource), f\"systemSource: '' is not a valid input\"\n",
    "assert bool(tableSource), f\"tableSource:  '' is not a valid input\"\n",
    "assert bool(trigger), f\"trigger:  '' is not a valid input\"\n",
    "assert bool(schemaName), f\"schemaName:  '' is not a valid input\"\n",
    "assert isDev != \"\" and (isDev in (\"True\",\"False\",\"0\",\"1\")), f\"isDev: {isDev} is not a valid input, expected True or False\"\n",
    "assert isIncremental != \"\" and (isIncremental in (\"True\",\"False\",\"0\",\"1\")), f\"isIncremental: {isIncremental} is not a valid input, expected True or False\"\n",
    "assert isSensitive != \"\" and (isSensitive in (\"True\",\"False\",\"0\",\"1\")), f\"isSensitive: {isSensitive} is not a valid input, expected True or False\"\n",
    "isDev = bool(eval(isDev))\n",
    "isIncremental = bool(eval(isIncremental))\n",
    "isSensitive = bool(eval(isSensitive))\n",
    "if not isDev:\n",
    "    environment = \"prod\"\n",
    "    control_sources = \"landing.control.sources\"\n",
    "    env_path = \"\"\n",
    "    control_incremental = \"landing.control.incremental\"\n",
    "    schema_adb = 'staging'\n",
    "    token_table = \"landing.control.token_table\"\n",
    "if isDev:\n",
    "    environment = \"dev\"\n",
    "    control_sources = \"landing.dev.sources\"\n",
    "    env_path = \"/dev\"  \n",
    "    control_incremental = \"landing.dev.incremental\"\n",
    "    schema_adb = 'dev'\n",
    "    token_table = \"landing.dev.token_table\"\n",
    "par_sources = spark.sql(f\"\"\"\n",
    "                   SELECT id, id_col, incr_col, incr_col_type, incr_size\n",
    "                   FROM {control_sources}\n",
    "                   WHERE \n",
    "                   systemSource  = '{systemSource}' AND\n",
    "                   tableSource   = '{tableSource}'  AND\n",
    "                   isIncremental = {isIncremental}  AND\n",
    "                   isSensitive   = {isSensitive}    AND\n",
    "                   isDev         = {isDev}          AND\n",
    "                   trigger       = '{trigger}'      AND\n",
    "                   schemaName    = '{schemaName}'\n",
    "                   LIMIT 1\n",
    "                   \"\"\")\n",
    "\n",
    "assert par_sources.count() == 1, f\"No Table Found in {control_sources} Based on the Chosen Criteria\"\n",
    "source_id, id_col, incr_col, incr_col_type, incr_size = par_sources.collect()[0]\n",
    "incremental = 'incremental' if isIncremental else 'snapshot'\n",
    "sensitive = 'sensitive' if isSensitive else 'general'\n",
    "merge_cols = eval(id_col)\n",
    "id_col = merge_cols[0]\n",
    "now = datetime.datetime.now()\n",
    "year_created, month_created, day_created, hour_created  =(now.year,now.month,now.day,now.hour)\n",
    "path_raw = f\"abfss://raw@storageaccount.dfs.core.windows.net{env_path}/{sensitive}/{systemSource}/{incremental}/{schemaName}/{tableSource}/{year_created}/{month_created}/{day_created}/{hour_created}/\"\n",
    "endpoint = tableSource\n",
    "api_url = \"***********************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(token_table):\n",
    "    spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {token_table}\n",
    "            (\n",
    "            id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "            token STRING,\n",
    "            date_generation TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    "            )\n",
    "            CLUSTER BY (date_generation)\n",
    "            TBLPROPERTIES(\n",
    "            'delta.feature.allowColumnDefaults' = 'supported',\n",
    "            'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "            'delta.autoOptimize.autoCompact' = 'true',\n",
    "            'deltaTable.deletedFileRetentionDuration' = 'interval 60 days',\n",
    "            'delta.timeUntilArchived' = '730 days',\n",
    "            'delta.enableDeletionVectors' = 'true'\n",
    "            )\n",
    "              \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    token = (spark.table(token_table)\n",
    "                .orderBy(desc(\"date_generation\"))\n",
    "                .select(\"token\")).first()[0]\n",
    "except:\n",
    "    token = 'init'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = api_url + endpoint\n",
    "headers = {\n",
    "    'Accept': 'application/json',\n",
    "    'X-AUTH-TOKEN': token\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'show_service': 'true',\n",
    "    'limit': 100000,\n",
    "    'page': 1,\n",
    "    'search': '{\"host.name\":{\"$lk\":\"%CLIENTE%\"}}'\n",
    "}\n",
    "\n",
    "request_data = []\n",
    "\n",
    "while True:\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 401:\n",
    "        get_token(environment,token_table)\n",
    "\n",
    "        token = (spark.table(token_table)\n",
    "            .orderBy(desc(\"date_generation\"))\n",
    "            .select(\"token\")).first()[0]\n",
    "        \n",
    "        headers['X-AUTH-TOKEN'] = token\n",
    "        continue \n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()['result']\n",
    "        if not data:\n",
    "            break  \n",
    "        request_data.extend(data)\n",
    "        params['page'] += 1  \n",
    "\n",
    "    else:\n",
    "        print(\"Request failed:\", response.text)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = sc.parallelize(request_data).map(lambda x: json.dumps(x))\n",
    "df = spark.read.json(json_df)\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"json\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_raw)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
