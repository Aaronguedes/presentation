{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime,timezone,timedelta\n",
    "from token_transformations import get_token\n",
    "from pyspark.sql.functions import collect_list,desc,col\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemSource = dbutils.widgets.get(\"systemSource\")\n",
    "tableSource = dbutils.widgets.get(\"tableSource\")\n",
    "isIncremental = dbutils.widgets.get(\"isIncremental\")\n",
    "isSensitive = dbutils.widgets.get(\"isSensitive\")\n",
    "isDev = dbutils.widgets.get(\"isDev\")\n",
    "trigger = dbutils.widgets.get(\"trigger\")\n",
    "schemaName = dbutils.widgets.get(\"schemaName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert None not in [systemSource,tableSource,isDev,isSensitive,isIncremental,schemaName], \"None is not a valid input\"\n",
    "isDev = str(isDev).strip().capitalize()\n",
    "tableSource = str(tableSource).strip()\n",
    "isSensitive = str(isSensitive).strip().capitalize()\n",
    "systemSource = str(systemSource).strip().lower()\n",
    "isIncremental = str(isIncremental).strip().capitalize()\n",
    "schemaName = str(schemaName).strip().lower()\n",
    "assert bool(systemSource), f\"systemSource: '' is not a valid input\"\n",
    "assert isSensitive != \"\" and (isSensitive in (\"True\",\"False\",\"0\",\"1\")), f\"isSensitive: {isSensitive} is not a valid input, expected True or False\"\n",
    "assert bool(tableSource), f\"tableSource:  '' is not a valid input\"\n",
    "assert isDev != \"\" and (isDev in (\"True\",\"False\",\"0\",\"1\")), f\"isDev: {isDev} is not a valid input, expected True or False\"\n",
    "assert isIncremental != \"\" and (isIncremental in (\"True\",\"False\",\"0\",\"1\")), f\"isIncremental: {isIncremental} is not a valid input, expected True or False\"\n",
    "assert bool(schemaName), f\"schemaName:  '' is not a valid input\"\n",
    "isDev = bool(eval(isDev))\n",
    "isSensitive = bool(eval(isSensitive))\n",
    "sensitive = 'sensitive' if isSensitive else 'general'\n",
    "isIncremental = bool(eval(isIncremental))\n",
    "incremental = 'incremental' if isIncremental else 'snapshot'\n",
    "if not isDev:\n",
    "    enviroment = \"prod\"\n",
    "    env_path = \"\"\n",
    "    schema_adb = 'staging'\n",
    "    token_table = \"landing.control.token_control\"\n",
    "if isDev:\n",
    "    enviroment = \"dev\"\n",
    "    env_path = \"/dev\"  \n",
    "    schema_adb = 'dev'\n",
    "    token_table = \"landing.dev.token_control\"\n",
    "\n",
    "now = datetime.now()\n",
    "year_created, month_created, day_created, hour_created, minute_created, second_created =(now.year,now.month,now.day,now.hour,now.minute,now.second)\n",
    "\n",
    "if tableSource == 'resources/hosts_services':\n",
    "    select_table = f'{enviroment}.silver.centreon_apirest_monitoring_hosts_service_id'\n",
    "\n",
    "elif tableSource == 'resources/hosts':\n",
    "    select_table =f'{enviroment}.silver.centreon_apirest_monitoring_hosts_id'\n",
    "\n",
    "path_raw = f\"abfss://landing@strawdpcdiaprodbrs.dfs.core.windows.net{env_path}/{sensitive}/{systemSource}/{incremental}/{schemaName}/{tableSource}/{year_created}/{month_created}/{day_created}/{hour_created}/{minute_created}/{second_created}\"\n",
    "tableSource = tableSource.replace('/','_')\n",
    "test_url = 'https://monitoramento.rnp.br/centreon/api/latest/monitoring/hosts/categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = (spark.table(token_table)\n",
    "            .orderBy(desc(\"date_generation\"))\n",
    "            .select(\"token\")).first()[0]\n",
    "headers = {\n",
    "            'Accept': 'application/json',\n",
    "            'X-AUTH-TOKEN': token\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_url = spark.table(select_table).select(collect_list(\"url\")).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##consulta assincrona\n",
    "\n",
    "nest_asyncio.apply()\n",
    "async def get_new_token():\n",
    "\n",
    "            get_token(enviroment,token_table)\n",
    "            token = (spark.table(token_table)\n",
    "            .orderBy(desc(\"date_generation\"))\n",
    "            .select(\"token\")).first()[0]\n",
    "            headers['X-AUTH-TOKEN'] = token\n",
    "\n",
    "async def test_token_validation(session, url, headers):\n",
    "    async with session.get(url, headers=headers) as resp:\n",
    "        return resp.status \n",
    "\n",
    "       \n",
    "async def fetch_data(session, url, headers):\n",
    "    try:\n",
    "        async with session.get(url, headers=headers) as resp:\n",
    "            if resp.status != 200:\n",
    "                return {\"_ingestion_url\": url, \"_ingestion_ts\": datetime.now(timezone(timedelta(hours=-3))).isoformat(),\"code\":resp.status}\n",
    "            \n",
    "            response = await resp.json(content_type=None)\n",
    "            \n",
    "            if response.get('code') == 404:\n",
    "                return {\"_ingestion_url\": url, \"_ingestion_ts\": datetime.now(timezone(timedelta(hours=-3))).isoformat(),\"code\":204}\n",
    "            response['_ingestion_url'] = url\n",
    "            response['_ingestion_ts'] = datetime.now(timezone(timedelta(hours=-3))).isoformat()\n",
    "            response['code'] = resp.status\n",
    "                \n",
    "        return response\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"An error occurred during the request: {e}, of {url}\")\n",
    "        return {\"_ingestion_url\": url, \"_ingestion_ts\":datetime.now(timezone(timedelta(hours=-3))).isoformat(),\"code\":resp.status}\n",
    "\n",
    "async def main():\n",
    "    conn = aiohttp.TCPConnector(limit=50,limit_per_host=0)\n",
    "    async with aiohttp.ClientSession(connector=conn) as session:\n",
    "\n",
    "        is_token_valid = await test_token_validation(session,test_url, headers)\n",
    "\n",
    "        if is_token_valid == 401:\n",
    "            await get_new_token()\n",
    "        tasks = [fetch_data(session, url, headers) for url in list_url]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "    \n",
    "results = asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = sc.parallelize(results).map(lambda x: json.dumps(x))\n",
    "df = spark.read.json(json_df)\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"json\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(key = \"ingestion_path\", value = path_raw)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
